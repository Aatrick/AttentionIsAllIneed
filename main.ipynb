{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embedding(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model=512) -> None:\n",
    "        super(Embedding, self).__init__()\n",
    "        self.vocab_size = vocab_size\n",
    "        self.d_model = d_model\n",
    "        # dmodel -> Dimension of model\n",
    "        self.embed_layer = nn.Embedding(vocab_size, d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # As author mentions -> In the embedding layers, we multiply those weights by sqrt(dmodel)  -> page 5\n",
    "        return self.embed_layer(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self,max_seq_len,d_model=512) -> None:\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        # create a matrix of positional encoding\n",
    "        pos = torch.arange(0, max_seq_len,dtype = torch.float).unsqueeze(1)\n",
    "        # we are dividing by 10000 because we know that we are using sin and cos function\n",
    "        # we know a^-x  is equals to 1/a^x\n",
    "        frequency = torch.pow(10000,-torch.arange(0, d_model, 2, dtype = torch.float)/self.d_model)\n",
    "        \n",
    "        # By alternating sine and cosine functions for even and odd dimensions, this encoding scheme ensures that each position in the sequence has a unique representation. \n",
    "        # This helps the transformer model to distinguish between different positions and capture the sequential nature of the data.\n",
    "        pe = torch.zeros((max_seq_len, d_model))\n",
    "        pe[:,0::2] = torch.sin(pos * frequency)\n",
    "        pe[:,1::2] = torch.cos(pos * frequency)\n",
    "        \n",
    "        # Here we use register_buffer, because it avoid update model parameter during backpropagation\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, embed_vect):\n",
    "        # add embedding output and positional encoding\n",
    "        return embed_vect + self.pe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=8, dropout_rate=0.2) -> None:\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        self.head_dim = d_model // n_heads\n",
    "        self.softmax_layer = nn.Softmax(dim=-1)\n",
    "        self.q_linear = nn.Linear(d_model, d_model)\n",
    "        self.v_linear = nn.Linear(d_model, d_model)\n",
    "        self.k_linear = nn.Linear(d_model, d_model)\n",
    "        self.out = nn.Linear(d_model, d_model)\n",
    "    \n",
    "    def attention(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "         BS: Batch Size - the number of sequences processed in parallel.\n",
    "         NH: Number of Heads - the number of attention heads in a multi-head attention mechanism.\n",
    "         S/T: Sequence Length or Target Length - the length of the input sequence (for encoder) or the target sequence (for decoder).\n",
    "         HD: Head Dimension - the dimensionality of each attention head.\n",
    "         \n",
    "         calculate attention score\n",
    "         query = (BS,NH,S/T,HD) , key.transpose(-2,-1) = (BS,NH,HD,S/T)\n",
    "         attention score size for encoder attention = (BS,NH,S,S) , decoder attention = (BS,NH,T,T), encoder-decoder attention = (BS,NH,T,S)\n",
    "        \"\"\"\n",
    "        # dot product of query and key\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(self.head_dim)\n",
    "        # apply mask if it is not None\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == torch.tensor(False), float(\"-inf\"))\n",
    "            \n",
    "        # pass through softmax layer\n",
    "        attention_weight = self.softmax_layer(scores)\n",
    "\n",
    "        # shape of output = (BS,NH,S/T,HD)\n",
    "        # return matrix multiplication of attention weight and value\n",
    "        return torch.matmul(attention_weight, value)\n",
    "    \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = key.size(0)\n",
    "        \n",
    "        \"\"\"\n",
    "         dot product with weight matrices\n",
    "         size of key/query/value = (BS,S/T,ED) ,\n",
    "         where BS = batch size,\n",
    "         S = Source Sequence length, T = target sequence length,\n",
    "         ED = Embedding dimension,\n",
    "         NH = Number Of Head, HD = head dimension\n",
    "        \"\"\"\n",
    "        key, query, value = self.k_linear(key), self.q_linear(query), self.v_linear(value)\n",
    "        \n",
    "        \"\"\"\n",
    "         split vector by number of head and transpose\n",
    "         size of key/query/value = (BS,NH,S/T,HD) , where BS = batch size, NH = Number Of Head, HD = Head dimension\n",
    "        \"\"\"\n",
    "        key = key.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        query = query.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        value = value.view(batch_size, -1, self.n_heads, self.head_dim).transpose(1, 2)\n",
    "        \n",
    "        # size of attention score = (BS,NH,S/T,HD)\n",
    "        attention_score = self.attention(query, key, value, mask) # size - torch.Size([2, 4, 8, 64]) -> [batch_size, max_seq_len, n_head, head_dim]\n",
    "        attention_score = self.dropout(attention_score)\n",
    "        \n",
    "        # concatenate heads and put through final linear layer\n",
    "        # size of output = (BS,S/T,NH,HD))\n",
    "        attention_score = attention_score.transpose(1, 2).reshape(batch_size, -1, self.head_dim * self.n_heads) # size = (BS, S/T, ED)\n",
    "        \n",
    "        return self.out(attention_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 5, section 3.3\n",
    "import torch.nn.functional as F\n",
    "class PositionWiseFeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    This consists of two linear transformations with a ReLU activation in between. \n",
    "    Purpose ot this layer is introduce non-linearity into the Transformer architecture, \n",
    "    allowing the model to learn more complex relationships between words and their positions within the sentence\n",
    "    \n",
    "    FFN(x) = max(0, xW1 + b1)W2 + b2\n",
    "\n",
    "    Args:\n",
    "        d_model (int, optional): [description]. Defaults to 512.\n",
    "        dropout_rate (float, optional): [description]. Defaults to 0.2.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, dropout_rate=0.2) -> None:\n",
    "        super(PositionWiseFeedForward, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        hidden_width = 4\n",
    "        self.linear1 = nn.Linear(d_model, d_model * hidden_width)\n",
    "        self.linear2 = nn.Linear(d_model * hidden_width, d_model)\n",
    "        self.dropout = nn.Dropout(p = dropout_rate)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.linear2(self.dropout(F.relu(self.linear1(x))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 3, section 3.1\n",
    "class SubLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    In Transformers, each encoder block consist two sub-layer: a multi-head attention mechanism and a position-wise feed-forward network (FFN). \n",
    "    That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model = 512) -> None:\n",
    "        super(SubLayer, self).__init__()\n",
    "        self.norm = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x, sub_layer_x):\n",
    "        return self.norm(x + sub_layer_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 3, section 3.1\n",
    "class EncoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Layer has two sub layer. The first is a multi-head self-attention mechanism, \n",
    "    and the second is a simple, position-wise fully connected feed-forward network.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model=512, n_heads=8, dropout_rate=0.2) -> None:\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        self.self_attention = MultiHeadAttention(d_model, n_heads, dropout_rate)\n",
    "        self.sub_layer1 = SubLayer(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        self.feed_forward = PositionWiseFeedForward(d_model, dropout_rate)\n",
    "        self.sub_layer2 = SubLayer(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self, vec_representation, src_mask=None):\n",
    "        \"\"\"\n",
    "        The output of the self-attention mechanism is passed through a feed-forward neural network, \n",
    "        which consists of two linear transformations with a ReLU activation in between.\n",
    "        \"\"\"\n",
    "        # pass through self-attention\n",
    "        attention_output = self.self_attention(key=vec_representation, query=vec_representation, value=vec_representation, mask=src_mask)\n",
    "        attention_output = self.dropout1(attention_output)\n",
    "\n",
    "        # pass through sub-layer 1\n",
    "        sub_layer1_output = self.sub_layer1(vec_representation, attention_output)\n",
    "        \n",
    "        # pass through feed forward network\n",
    "        feed_forward_output = self.feed_forward(sub_layer1_output)\n",
    "        feed_forward_output = self.dropout2(feed_forward_output)\n",
    "        \n",
    "        # pass through sub-layer 2\n",
    "        return self.sub_layer2(sub_layer1_output, feed_forward_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Page 3, section 3.1\n",
    "class EncoderBlock(nn.Module):\n",
    "    #The encoder is composed of a stack of N = 6 identical layers.\n",
    "    def __init__(self, encoder_layer, n_layers=6) -> None:\n",
    "        super(EncoderBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(encoder_layer) for _ in range(n_layers)])\n",
    "    \n",
    "    def forward(self, src_embedding, src_mask=None):\n",
    "        for layer in self.layers:\n",
    "            src_embedding = layer(src_embedding, src_mask)\n",
    "        return src_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    \"\"\"\n",
    "    Encoder Layer has three sub layer. \n",
    "    In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, \n",
    "    which performs multi-head attention over the output of the encoder stack.\n",
    "\n",
    "    Decoder layer contain 2 layer, one is Mask Multi-Head Attention another one is “encoder-decoder attention”. \n",
    "    In “encoder-decoder attention” layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder. \n",
    "    This allows every position in the decoder to attend over all positions in the input sequence.\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, multi_head_attention_layer, position_wise_feedforward_layer, dropout_rate = 0.2) -> None:\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.decoder_attention_layer = copy.deepcopy(multi_head_attention_layer)\n",
    "        self.sub_layer1 = SubLayer(d_model)\n",
    "        self.dropout1 = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        self.encoder_decoder_attention_layer = copy.deepcopy(multi_head_attention_layer)\n",
    "        self.sub_layer2 = SubLayer(d_model)\n",
    "        self.dropout2 = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "        self.feed_forward = copy.deepcopy(position_wise_feedforward_layer)\n",
    "        self.sub_layer3 = SubLayer(d_model)\n",
    "        self.dropout3 = nn.Dropout(p=dropout_rate)\n",
    "\n",
    "    def forward(self,enc,dec,src_mask=None,target_mask=None):\n",
    "        \"\"\"\n",
    "        The decoder layer is composed of three sub-layers. \n",
    "        The first is a multi-head self-attention mechanism, the second is a multi-head attention mechanism, \n",
    "        and the third is a simple, position-wise fully connected feed-forward network.\n",
    "        \"\"\"\n",
    "        # pass through self-attention\n",
    "        decoder_attention_output = self.decoder_attention_layer(key=dec, query=dec, value=dec, mask=target_mask)\n",
    "        decoder_attention_output = self.dropout1(decoder_attention_output)\n",
    "\n",
    "        # pass through sub-layer 1\n",
    "        sub_layer1_output = self.sub_layer1(dec, decoder_attention_output)\n",
    "\n",
    "        # pass through encoder-decoder attention\n",
    "        encoder_decoder_attention_output = self.encoder_decoder_attention_layer(key=enc, query=sub_layer1_output, value=enc, mask=src_mask)\n",
    "        encoder_decoder_attention_output = self.dropout2(encoder_decoder_attention_output)\n",
    "\n",
    "        # pass through sub-layer 2\n",
    "        sub_layer2_output = self.sub_layer2(sub_layer1_output, encoder_decoder_attention_output)\n",
    "\n",
    "        # pass through feed forward network\n",
    "        feed_forward_output = self.feed_forward(sub_layer2_output)\n",
    "        feed_forward_output = self.dropout3(feed_forward_output)\n",
    "\n",
    "        # pass through sub-layer 3\n",
    "        return self.sub_layer3(sub_layer2_output, feed_forward_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderBlock(nn.Module):\n",
    "    # The decoder is also composed of a stack of N = 6 identical layers.\n",
    "    def __init__(self, decoder_layer, n_layers=6) -> None:\n",
    "        super(DecoderBlock, self).__init__()\n",
    "        self.layers = nn.ModuleList([copy.deepcopy(decoder_layer) for _ in range(n_layers)])\n",
    "        self.layer_norm = nn.LayerNorm(decoder_layer.d_model)\n",
    "    \n",
    "    def forward(self, encoder_out_vec, decoder_embedding, src_mask=None, target_mask=None):\n",
    "        for layer in self.layers:\n",
    "            decoder_embedding = layer(enc = encoder_out_vec, dec = decoder_embedding, src_mask =  src_mask, target_mask = target_mask)\n",
    "        return decoder_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderGenerator(nn.Module):\n",
    "    # In this layer we generating the output sequence one token at a time.\n",
    "    # The output of the final decoder layer is passed through a linear transformation and a softmax function to generate the output sequence.\n",
    "    def __init__(self, d_model, vocab_size) -> None:\n",
    "        super(DecoderGenerator, self).__init__()\n",
    "        self.linear = nn.Linear(d_model, vocab_size)\n",
    "        self.softmax = nn.LogSoftmax(dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.softmax(self.linear(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformers(nn.Module):\n",
    "    # merging all the layers together\n",
    "    def __init__(self,src_seq_len,trg_seq_len,d_model,num_head,dropout_rate = 0.2) -> None:\n",
    "        super(Transformers, self).__init__()\n",
    "        self.src_seq_len = src_seq_len\n",
    "        self.trg_seq_len = trg_seq_len\n",
    "        self.d_model = d_model\n",
    "        self.num_head = num_head\n",
    "\n",
    "        self.src_embedding = Embedding(src_seq_len, d_model)\n",
    "        self.src_positional_encoding = PositionalEncoding(src_seq_len, d_model)\n",
    "\n",
    "        self.trg_embedding = Embedding(trg_seq_len, d_model)\n",
    "        self.trg_positional_encoding = PositionalEncoding(trg_seq_len, d_model)\n",
    "\n",
    "        self.multi_head_attention = MultiHeadAttention(d_model, num_head, dropout_rate)\n",
    "        self.position_wise_feedforward = PositionWiseFeedForward(d_model, dropout_rate)\n",
    "\n",
    "        self.encoder_layer = EncoderLayer(d_model, num_head, dropout_rate)\n",
    "        self.encoder_block = EncoderBlock(self.encoder_layer)\n",
    "\n",
    "        self.decoder_layer = DecoderLayer(d_model, self.multi_head_attention, self.position_wise_feedforward, dropout_rate)\n",
    "        self.decoder_block = DecoderBlock(self.decoder_layer)\n",
    "\n",
    "        self.decoder_generator = DecoderGenerator(d_model, trg_seq_len)\n",
    "\n",
    "    def forward(self,src_token_id,target_token_id,src_mask=None,target_mask=None):\n",
    "        encode_out = self.encode(src_token_id, src_mask)\n",
    "        decode_out = self.decode(target_token_id, encode_out, src_mask, target_mask)\n",
    "        return decode_out\n",
    "    \n",
    "    def encode(self,src_token_id,src_mask):\n",
    "        src_embedding = self.src_embedding(src_token_id)\n",
    "        src_embedding = self.src_positional_encoding(src_embedding)\n",
    "        return self.encoder_block(src_embedding, src_mask)\n",
    "    \n",
    "    def decode(self,target_token_id,encode_out,src_mask,target_mask):\n",
    "        embed = self.src_embedding(target_token_id)\n",
    "        pe_out = self.src_positional_encoding(embed)\n",
    "        decode_out = self.decoder_block(encode_out, pe_out, src_mask, target_mask)\n",
    "        return self.decoder_generator(decode_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_src_mask(src_token_ids_batch,pad_tok_id):\n",
    "    batch_size = src_token_ids_batch.size()[0]\n",
    "    src_mask = (src_token_ids_batch!=pad_tok_id).view(batch_size, 1, 1,-1) #SIZE = (BS,1,1,S)\n",
    "    return src_mask\n",
    "def get_trg_mask(trg_token_ids_batch,pad_tok_id):\n",
    "    batch_size = trg_token_ids_batch.size()[0]\n",
    "    seq_len = trg_token_ids_batch.size()[1]\n",
    "    trg_pad_mask = (trg_token_ids_batch!=pad_tok_id).view(batch_size, 1, 1,-1) #SIZE = (BS,1,1,T)\n",
    "    trg_look_forward = torch.triu(torch.ones(1,1,seq_len,seq_len, dtype=torch.bool)).transpose(2,3)\n",
    "    trg_mask = trg_pad_mask & trg_look_forward\n",
    "    return trg_mask\n",
    "\n",
    "UNK_IDX, PAD_IDX, BOS_IDX, EOS_IDX = 0, 1, 2, 3\n",
    "low_bound = 3\n",
    "high_bound = 15\n",
    "batch_size = 32\n",
    "# Update src_seq_len and trg_seq_len to match the expected sequence length\n",
    "src_seq_len = 20\n",
    "trg_seq_len = 20\n",
    "\n",
    "# Update tensor sizes accordingly\n",
    "src_tensor_size = (batch_size, src_seq_len)\n",
    "trg_tensor_size = (batch_size, trg_seq_len)\n",
    "\n",
    "# Generate source and target sequences with the updated sizes\n",
    "src_seq = torch.randint(3, 16, size=src_tensor_size, dtype=torch.int32)\n",
    "trg_seq = torch.randint(3, 16, size=trg_tensor_size, dtype=torch.int32)\n",
    "\n",
    "# Initialize the transformer with the updated sequence lengths\n",
    "transformer = Transformers(\n",
    "    src_seq_len=src_seq_len,\n",
    "    trg_seq_len=trg_seq_len,\n",
    "    d_model=512,\n",
    "    num_head=8,\n",
    "    dropout_rate=0.2\n",
    ")\n",
    "\n",
    "# Generate masks\n",
    "src_mask = get_src_mask(src_seq, PAD_IDX)\n",
    "trg_mask = get_trg_mask(trg_seq, PAD_IDX)\n",
    "\n",
    "# Forward pass\n",
    "output = transformer(src_seq, trg_seq, src_mask, trg_mask)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
